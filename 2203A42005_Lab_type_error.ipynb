{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUDEVZQnoQBy",
        "outputId": "d83607ba-a69c-49c9-d546-5f163fc12222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to your zip file\n",
        "zip_path = \"/content/drive/MyDrive/Early Stage Diabetes Risk Prediction Dataset.zip\"\n",
        "\n",
        "# Destination folder to extract files\n",
        "extract_path = \"/content/Early Stage Diabetes Risk Prediction Dataset\"\n",
        "\n",
        "# Ensure the extraction directory exists\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Train Logistic Regression, Decision Tree, and Random Forest models on the dataset. Compare their accuracy, precision, and recall.\n"
      ],
      "metadata": {
        "id": "AXTlUeofvsVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "\n",
        "# Load dataset (assuming CSV file is extracted)\n",
        "csv_path = os.path.join(extract_path, \"/content/Early Stage Diabetes Risk Prediction Dataset/diabetes_data_upload.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Splitting data\n",
        "X = df.drop(columns=['class'])  # Assuming 'class' is the target column\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])  # False Negative Rate (Type II Error)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"False Negative Rate\": fn_rate\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.sort_values(by=\"Accuracy\", ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LBWhFblokpC",
        "outputId": "1b0d1f89-582d-4ec0-df93-8f5a77222fb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Accuracy  Precision    Recall  False Negative Rate\n",
            "2        Random Forest  0.990385   1.000000  0.985915             0.014085\n",
            "1        Decision Tree  0.961538   1.000000  0.943662             0.056338\n",
            "0  Logistic Regression  0.923077   0.931507  0.957746             0.042254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Train a Logistic Regression model and perform a Z-Test on the mean age of correctly classified vs. misclassified diabetic patients."
      ],
      "metadata": {
        "id": "n_vt7vrav3Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "# Load dataset (assuming CSV file is extracted)\n",
        "csv_path = os.path.join(extract_path, \"/content/Early Stage Diabetes Risk Prediction Dataset/diabetes_data_upload.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' and col != 'Age':  # Keep 'Age' as numeric\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Splitting data\n",
        "X = df.drop(columns=['class'])  # Assuming 'class' is the target column\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Standardizing features (excluding Age for interpretation purposes)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(columns=[\"Age\"]))\n",
        "X_test_scaled = scaler.transform(X_test.drop(columns=[\"Age\"]))\n",
        "\n",
        "# Convert back to DataFrame and restore Age column\n",
        "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns[1:], index=X_train.index)\n",
        "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns[1:], index=X_test.index)\n",
        "\n",
        "# Reattach the Age column\n",
        "X_train.insert(0, \"Age\", df.loc[X_train.index, \"Age\"])\n",
        "X_test.insert(0, \"Age\", df.loc[X_test.index, \"Age\"])\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Get correctly and incorrectly classified diabetic patients\n",
        "test_results = pd.DataFrame({\n",
        "    \"Age\": X_test[\"Age\"].values,\n",
        "    \"True_Label\": y_test.values,\n",
        "    \"Predicted_Label\": y_pred\n",
        "})\n",
        "\n",
        "test_results_diabetic = test_results[test_results[\"True_Label\"] == 1]\n",
        "correctly_classified = test_results_diabetic[test_results_diabetic[\"True_Label\"] == test_results_diabetic[\"Predicted_Label\"]]\n",
        "misclassified = test_results_diabetic[test_results_diabetic[\"True_Label\"] != test_results_diabetic[\"Predicted_Label\"]]\n",
        "\n",
        "# Perform Z-Test on mean age of correctly vs. misclassified diabetic patients\n",
        "z_stat, p_value = stats.ttest_ind(correctly_classified[\"Age\"], misclassified[\"Age\"], equal_var=False)\n",
        "\n",
        "# Print results\n",
        "print(f\"Z-Statistic: {z_stat}, P-Value: {p_value}\")\n",
        "if p_value < 0.05:\n",
        "    print(\"Significant difference in mean age. Age is likely an important feature.\")\n",
        "else:\n",
        "    print(\"No significant difference in mean age. Age may not be a strong factor.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4QHKORep6kh",
        "outputId": "2c68444f-41a0-4615-b017-155fedd43085"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z-Statistic: 5.448854897317102, P-Value: 1.2770382292057548e-05\n",
            "Significant difference in mean age. Age is likely an important feature.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Train a Random Forest model and analyze its false positive rate (Type I error)."
      ],
      "metadata": {
        "id": "3z5tYhdvwEwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "# Load dataset (assuming CSV file is extracted)\n",
        "csv_path = os.path.join(extract_path, \"/content/Early Stage Diabetes Risk Prediction Dataset/diabetes_data_upload.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' and col != 'Age':  # Keep 'Age' as numeric\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Splitting data\n",
        "X = df.drop(columns=['class'])  # Assuming 'class' is the target column\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing features (excluding Age for interpretation purposes)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(columns=[\"Age\"]))\n",
        "X_test_scaled = scaler.transform(X_test.drop(columns=[\"Age\"]))\n",
        "\n",
        "# Convert back to DataFrame and restore Age column\n",
        "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns[1:], index=X_train.index)\n",
        "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns[1:], index=X_test.index)\n",
        "X_train.insert(0, \"Age\", df.loc[X_train.index, \"Age\"])\n",
        "X_test.insert(0, \"Age\", df.loc[X_test.index, \"Age\"])\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Compute False Positive Rate (Type I Error)\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "fp_rate_rf = cm_rf[0][1] / (cm_rf[0][0] + cm_rf[0][1])\n",
        "print(f\"Random Forest False Positive Rate: {fp_rate_rf:.4f}\")\n",
        "\n",
        "# If False Positive Rate > 20%, perform a One-Sample Z-Test\n",
        "alpha = 0.05  # Significance level\n",
        "threshold_fp_rate = 0.20\n",
        "if fp_rate_rf > threshold_fp_rate:\n",
        "    sample_size = cm_rf[0][0] + cm_rf[0][1]  # Total non-diabetic samples\n",
        "    z_stat, p_value = stats.ttest_1samp([fp_rate_rf] * sample_size, threshold_fp_rate)\n",
        "    print(f\"Z-Statistic: {z_stat}, P-Value: {p_value}\")\n",
        "    if p_value < alpha:\n",
        "        print(\"False positive rate is significantly different from 20%.\")\n",
        "    else:\n",
        "        print(\"False positive rate is not significantly different from 20%.\")\n",
        "\n",
        "# Train Gradient Boosting Model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Compute False Positive Rate for Gradient Boosting\n",
        "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
        "fp_rate_gb = cm_gb[0][1] / (cm_gb[0][0] + cm_gb[0][1])\n",
        "print(f\"Gradient Boosting False Positive Rate: {fp_rate_gb:.4f}\")\n",
        "\n",
        "# Compare Models\n",
        "if fp_rate_rf < fp_rate_gb:\n",
        "    print(\"Random Forest has a lower Type I error rate.\")\n",
        "elif fp_rate_gb < fp_rate_rf:\n",
        "    print(\"Gradient Boosting has a lower Type I error rate.\")\n",
        "else:\n",
        "    print(\"Both models have the same Type I error rate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqzkpGLhqOc8",
        "outputId": "0c1a170a-145c-46d2-cef5-01cbeb7b740f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest False Positive Rate: 0.0000\n",
            "Gradient Boosting False Positive Rate: 0.0000\n",
            "Both models have the same Type I error rate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Compare the false negative rates (Type II errors) of SVM, KNN, and Logistic Regression models."
      ],
      "metadata": {
        "id": "PFnUouyowNjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# Load dataset (assuming CSV file is extracted)\n",
        "csv_path = os.path.join(extract_path, \"/content/Early Stage Diabetes Risk Prediction Dataset/diabetes_data_upload.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' and col != 'Age':  # Keep 'Age' as numeric\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Splitting data\n",
        "X = df.drop(columns=['class'])  # Assuming 'class' is the target column\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing features (excluding Age for interpretation purposes)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(columns=[\"Age\"]))\n",
        "X_test_scaled = scaler.transform(X_test.drop(columns=[\"Age\"]))\n",
        "\n",
        "# Convert back to DataFrame and restore Age column\n",
        "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns[1:], index=X_train.index)\n",
        "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns[1:], index=X_test.index)\n",
        "X_train.insert(0, \"Age\", df.loc[X_train.index, \"Age\"])\n",
        "X_test.insert(0, \"Age\", df.loc[X_test.index, \"Age\"])\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Train KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg_model = LogisticRegression(random_state=42)\n",
        "logreg_model.fit(X_train, y_train)\n",
        "y_pred_logreg = logreg_model.predict(X_test)\n",
        "\n",
        "# Compute False Negative Rates (Type II Error)\n",
        "def compute_false_negative_rate(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
        "    return fn_rate, cm\n",
        "\n",
        "fn_rate_svm, cm_svm = compute_false_negative_rate(y_test, y_pred_svm)\n",
        "fn_rate_knn, cm_knn = compute_false_negative_rate(y_test, y_pred_knn)\n",
        "fn_rate_logreg, cm_logreg = compute_false_negative_rate(y_test, y_pred_logreg)\n",
        "\n",
        "print(f\"SVM False Negative Rate: {fn_rate_svm:.4f}\")\n",
        "print(f\"KNN False Negative Rate: {fn_rate_knn:.4f}\")\n",
        "print(f\"Logistic Regression False Negative Rate: {fn_rate_logreg:.4f}\")\n",
        "\n",
        "# Perform Z-Test to compare false negative rates\n",
        "alpha = 0.05  # Significance level\n",
        "def z_test(fn_rate1, fn_rate2, n):\n",
        "    prop_diff = fn_rate1 - fn_rate2\n",
        "    std_error = np.sqrt((fn_rate1 * (1 - fn_rate1) / n) + (fn_rate2 * (1 - fn_rate2) / n))\n",
        "    z_stat = prop_diff / std_error\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed test\n",
        "    return z_stat, p_value\n",
        "\n",
        "# Assuming equal sample size for both groups\n",
        "n_samples = len(y_test) // 2  # Approximate number of diabetic cases\n",
        "\n",
        "z_svm_knn, p_svm_knn = z_test(fn_rate_svm, fn_rate_knn, n_samples)\n",
        "z_svm_logreg, p_svm_logreg = z_test(fn_rate_svm, fn_rate_logreg, n_samples)\n",
        "z_knn_logreg, p_knn_logreg = z_test(fn_rate_knn, fn_rate_logreg, n_samples)\n",
        "\n",
        "print(f\"Z-Test (SVM vs KNN): Z-Statistic = {z_svm_knn:.4f}, P-Value = {p_svm_knn:.4f}\")\n",
        "print(f\"Z-Test (SVM vs Logistic Regression): Z-Statistic = {z_svm_logreg:.4f}, P-Value = {p_svm_logreg:.4f}\")\n",
        "print(f\"Z-Test (KNN vs Logistic Regression): Z-Statistic = {z_knn_logreg:.4f}, P-Value = {p_knn_logreg:.4f}\")\n",
        "\n",
        "# Determine the best model based on Type II error\n",
        "best_model = min([(fn_rate_svm, \"SVM\"), (fn_rate_knn, \"KNN\"), (fn_rate_logreg, \"Logistic Regression\")])[1]\n",
        "print(f\"The model with the lowest False Negative Rate is: {best_model}\")\n",
        "\n",
        "# Recommendation for real-world deployment\n",
        "if fn_rate_logreg <= fn_rate_svm and fn_rate_logreg <= fn_rate_knn:\n",
        "    print(\"Logistic Regression is recommended for real-world deployment due to its lower False Negative Rate, reducing the risk of undiagnosed diabetes cases.\")\n",
        "else:\n",
        "    print(f\"{best_model} is recommended for real-world deployment due to its lower False Negative Rate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXnQ_TeAtGGN",
        "outputId": "bd81d3bf-dcde-4404-a018-d1c1347d241e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM False Negative Rate: 0.0845\n",
            "KNN False Negative Rate: 0.1549\n",
            "Logistic Regression False Negative Rate: 0.0423\n",
            "Z-Test (SVM vs KNN): Z-Statistic = -1.1127, P-Value = 0.2658\n",
            "Z-Test (SVM vs Logistic Regression): Z-Statistic = 0.8876, P-Value = 0.3747\n",
            "Z-Test (KNN vs Logistic Regression): Z-Statistic = 1.9626, P-Value = 0.0497\n",
            "The model with the lowest False Negative Rate is: Logistic Regression\n",
            "Logistic Regression is recommended for real-world deployment due to its lower False Negative Rate, reducing the risk of undiagnosed diabetes cases.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Train a Gradient Boosting Model and examine the misclassification of diabetic patients.\n"
      ],
      "metadata": {
        "id": "JcLrAaDXwTua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load dataset (assuming CSV file is extracted)\n",
        "csv_path = os.path.join(extract_path, \"/content/Early Stage Diabetes Risk Prediction Dataset/diabetes_data_upload.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' and col != 'Age':  # Keep 'Age' as numeric\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Splitting data\n",
        "X = df.drop(columns=['class'])  # Assuming 'class' is the target column\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing features (excluding Age for interpretation purposes)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(columns=[\"Age\"]))\n",
        "X_test_scaled = scaler.transform(X_test.drop(columns=[\"Age\"]))\n",
        "\n",
        "# Convert back to DataFrame and restore Age column\n",
        "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns[1:], index=X_train.index)\n",
        "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns[1:], index=X_test.index)\n",
        "X_train.insert(0, \"Age\", df.loc[X_train.index, \"Age\"])\n",
        "X_test.insert(0, \"Age\", df.loc[X_test.index, \"Age\"])\n",
        "\n",
        "# Train Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Compute False Negative Rates (Type II Error)\n",
        "def compute_false_negative_rate(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fn_rate = cm[1][0] / (cm[1][0] + cm[1][1])\n",
        "    return fn_rate, cm\n",
        "\n",
        "fn_rate_gb, cm_gb = compute_false_negative_rate(y_test, y_pred_gb)\n",
        "fn_rate_rf, cm_rf = compute_false_negative_rate(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Gradient Boosting False Negative Rate: {fn_rate_gb:.4f}\")\n",
        "print(f\"Random Forest False Negative Rate: {fn_rate_rf:.4f}\")\n",
        "\n",
        "# Perform Z-Test to compare misclassified ages\n",
        "misclassified_gb = X_test[(y_test != y_pred_gb)]['Age']\n",
        "correctly_classified_gb = X_test[(y_test == y_pred_gb)]['Age']\n",
        "\n",
        "z_stat, p_value = stats.ttest_ind(misclassified_gb, correctly_classified_gb, equal_var=False)\n",
        "\n",
        "print(f\"Z-Test for Age (Gradient Boosting Misclassified vs Correctly Classified): Z-Statistic = {z_stat:.4f}, P-Value = {p_value:.4f}\")\n",
        "\n",
        "# Recommendation based on Type II errors\n",
        "best_model = \"Gradient Boosting\" if fn_rate_gb < fn_rate_rf else \"Random Forest\"\n",
        "print(f\"The model with the lowest False Negative Rate is: {best_model}\")\n",
        "\n",
        "if fn_rate_gb < fn_rate_rf:\n",
        "    print(\"Gradient Boosting is recommended for medical use due to its lower False Negative Rate, reducing the risk of undiagnosed diabetes cases.\")\n",
        "else:\n",
        "    print(\"Random Forest is recommended for medical use due to its lower False Negative Rate, reducing the risk of undiagnosed diabetes cases.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BvZOKb8to0W",
        "outputId": "5ee95ff5-40ff-4a80-a615-ac95092db719"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting False Negative Rate: 0.0423\n",
            "Random Forest False Negative Rate: 0.0141\n",
            "Z-Test for Age (Gradient Boosting Misclassified vs Correctly Classified): Z-Statistic = -1.8681, P-Value = 0.1928\n",
            "The model with the lowest False Negative Rate is: Random Forest\n",
            "Random Forest is recommended for medical use due to its lower False Negative Rate, reducing the risk of undiagnosed diabetes cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Train three different models (e.g., Logistic Regression, SVM, Random Forest) and compare their Type I and Type II error rates."
      ],
      "metadata": {
        "id": "fN2x-kW2wY_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from scipy import stats\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# Load dataset (assuming CSV file is extracted)\n",
        "csv_path = os.path.join(extract_path, \"/content/Early Stage Diabetes Risk Prediction Dataset/diabetes_data_upload.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' and col != 'Age':  # Keep 'Age' as numeric\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Splitting data\n",
        "X = df.drop(columns=['class'])  # Assuming 'class' is the target column\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing features (excluding Age for interpretation purposes)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(columns=[\"Age\"]))\n",
        "X_test_scaled = scaler.transform(X_test.drop(columns=[\"Age\"]))\n",
        "\n",
        "# Convert back to DataFrame and restore Age column\n",
        "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns[1:], index=X_train.index)\n",
        "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns[1:], index=X_test.index)\n",
        "X_train.insert(0, \"Age\", df.loc[X_train.index, \"Age\"])\n",
        "X_test.insert(0, \"Age\", df.loc[X_test.index, \"Age\"])\n",
        "\n",
        "# Train models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    type_i_error = cm[0][1] / (cm[0][0] + cm[0][1])  # False positive rate\n",
        "    type_ii_error = cm[1][0] / (cm[1][0] + cm[1][1])  # False negative rate\n",
        "\n",
        "    results[name] = {\n",
        "        \"Type I Error\": type_i_error,\n",
        "        \"Type II Error\": type_ii_error,\n",
        "        \"False Positives\": cm[0][1],\n",
        "        \"False Negatives\": cm[1][0],\n",
        "        \"Total Positives\": cm[0][0] + cm[0][1],\n",
        "        \"Total Negatives\": cm[1][0] + cm[1][1]\n",
        "    }\n",
        "\n",
        "    print(f\"{name} - Type I Error: {type_i_error:.4f}, Type II Error: {type_ii_error:.4f}\")\n",
        "\n",
        "# Perform Two-Proportion Z-Test for statistical significance\n",
        "model_pairs = [(\"Logistic Regression\", \"SVM\"), (\"Logistic Regression\", \"Random Forest\"), (\"SVM\", \"Random Forest\")]\n",
        "\n",
        "for model1, model2 in model_pairs:\n",
        "    for error_type in [\"Type I Error\", \"Type II Error\"]:\n",
        "        count = np.array([results[model1][\"False Positives\" if error_type == \"Type I Error\" else \"False Negatives\"],\n",
        "                          results[model2][\"False Positives\" if error_type == \"Type I Error\" else \"False Negatives\"]])\n",
        "\n",
        "        nobs = np.array([results[model1][\"Total Positives\" if error_type == \"Type I Error\" else \"Total Negatives\"],\n",
        "                         results[model2][\"Total Positives\" if error_type == \"Type I Error\" else \"Total Negatives\"]])\n",
        "\n",
        "        if min(nobs) > 0:  # Avoid divide by zero error\n",
        "            z_stat, p_value = proportions_ztest(count, nobs)\n",
        "            print(f\"Z-Test for {error_type} ({model1} vs {model2}): Z-Statistic = {z_stat:.4f}, P-Value = {p_value:.4f}\")\n",
        "        else:\n",
        "            print(f\"Z-Test for {error_type} ({model1} vs {model2}): Not enough data for statistical test.\")\n",
        "\n",
        "# Select the best model for medical use (minimizing Type II Error)\n",
        "best_model = min(results, key=lambda x: results[x][\"Type II Error\"])\n",
        "print(f\"Recommended model for medical use: {best_model} due to lowest Type II Error.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxzqdHPcuZ62",
        "outputId": "cdce58db-6652-4b5a-ac08-05efa1b0b8bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Type I Error: 0.1515, Type II Error: 0.0423\n",
            "SVM - Type I Error: 0.3030, Type II Error: 0.0704\n",
            "Random Forest - Type I Error: 0.0000, Type II Error: 0.0141\n",
            "Z-Test for Type I Error (Logistic Regression vs SVM): Z-Statistic = -1.4686, P-Value = 0.1419\n",
            "Z-Test for Type II Error (Logistic Regression vs SVM): Z-Statistic = -0.7279, P-Value = 0.4667\n",
            "Z-Test for Type I Error (Logistic Regression vs Random Forest): Z-Statistic = 2.3259, P-Value = 0.0200\n",
            "Z-Test for Type II Error (Logistic Regression vs Random Forest): Z-Statistic = 1.0144, P-Value = 0.3104\n",
            "Z-Test for Type I Error (SVM vs Random Forest): Z-Statistic = 3.4330, P-Value = 0.0006\n",
            "Z-Test for Type II Error (SVM vs Random Forest): Z-Statistic = 1.6686, P-Value = 0.0952\n",
            "Recommended model for medical use: Random Forest due to lowest Type II Error.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFPWO5j_vR3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}